```mermaid
graph TD
    A[Input (3xH xW)] --> B[Encoder]
    B --> B0[Conv2d(3->16, k=3, s=2) + BN + Hardswish]
    B --> B1[InvertedResidual(16->16, k=3, s=2, SE, ReLU)]
    B --> B2[InvertedResidual(16->24, exp=72, k=3, s=2)]
    B --> B3[InvertedResidual(24->24, exp=88, k=3, s=1)]
    B --> B4[InvertedResidual(24->40, exp=96, k=5, s=2, SE)]
    B --> B5[InvertedResidual(40->40, exp=240, k=5, s=1, SE)]
    B --> B6[InvertedResidual(40->40, exp=240, k=5, s=1, SE)]
    B --> B7[InvertedResidual(40->48, exp=120, k=5, s=1, SE)]
    B --> B8[InvertedResidual(48->48, exp=144, k=5, s=1, SE)]
    B --> B9[InvertedResidual(48->96, exp=288, k=5, s=2, SE)]
    B --> B10[InvertedResidual(96->96, exp=576, k=5, s=1, SE)]
    B --> B11[InvertedResidual(96->96, exp=576, k=5, s=1, SE)]
    B --> B12[Conv2d(96->576, k=1, s=1) + BN + Hardswish]
    B --> C[Projection Layer: Conv2d(576->128, k=1, s=1)]
    C --> D[LinearAttentionBlock (128 channels)]
    D --> E[Decoder]
    E --> E0[ConvTranspose2d(128->64, k=2, s=2) + BN + ReLU]
    E --> E1[ConvTranspose2d(64->32, k=2, s=2) + BN + ReLU]
    E --> E2[Conv2d(32->10, k=1, s=1)]
    E --> F[PrototypeAttention]
    F --> G[Output (10 x H_out x W_out)]
